{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding a Logistic Regression model in Python\n",
    "\n",
    "Logistic Regression is a type of probabilistic classification model. It works best for a binary output (e.g., predicting whether someone has a disease or not; predicting if a credit card transaction is likely to be fraudulent or not; predicting whether someone is likely to survive the Titanic or not). \n",
    "\n",
    "## The difference between probability and likelihood\n",
    "\n",
    "### Probability\n",
    "\n",
    "Probability refers to finding the chance that something happens given a sample distribution of data. For example, if we collected income data from 100 people which was normally distributed and had the following characteristics: $$ \\mu = 50,000 $$ and $$ \\sigma = 5 ,000$$ we could calculate the *probability* that someone has a salary greater than $48,000. First we find the z-score: $$ z = {x - \\mu \\over \\sigma} $$ thus $$ z = {48,000 - 50,000 \\over 5,000} $$ which results in $$ z = -0.4 $$\n",
    "\n",
    "Then, we can look up the corresponding z-value in the z-table which equals 0.34458. Now we can calculate the probability that someone makes more than $48,000 by calculating $$P(>48,000) = 1-0.34458 = 0.65542$$ Therefore, there is a 65% chance a randomly selected person makes more than $48,000\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "*Likelihood* flips the question around and essentially tries to figure out the best characteristics (probability distribution) of data given a particular value. For example, if we collected a bunch of income data (let's say 100 observations), and we observed that most of the observations are between $45,000 and $50,000, which distribution (normal, exponential, gamma) with what characteristics (mean, standard deviation) would maximize the likelihood that we observe what we observed? We could check a whole bunch of different distributions (e.g., normal distribution with mean of $30,000 and stdev of $2,000, OR normal distribution with mean of $48,000 and stdev of $3,000, and so on) and how they fit the data, and we would pick the distribution and characteristics which maximize the likelihood that we see what we saw in our data (i.e., if our collected data had an average income of $48,000, we would pick the distribution which has this average as well). Basically, we are trying to find the best distribution as opposed to already having the distribution available. Once we find the best distribution, we can calculate probabilities!\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "In the real world, a common modelling problem is the question of estimating a joint probability distribution for a dataset. Density estimation would involve selecting a probability distribution function and the parameters that best explain the joint probability of observed data, so the obvious questions are: How should you choose the distribution function? How should you choose the parameters for the probability density function?\n",
    "\n",
    "Two common techniques are Maximum a Posteriori (MAP) and Maximum Likelihood Estimation (MLE). MLE assumes that all solutions are equally likely, while MAP takes into account prior information about the form of the solution.\n",
    "\n",
    "MLE treats the problem as an optimization problem (i.e., searching for the optimal solution). We wish to maximize the probability of observing the data from a joint probability distribution given a distribution and parameters that we already know. Mathematically this is stated as:\n",
    "\n",
    "$$ P(X | \\theta)$$ \n",
    "\n",
    "where theta is an unknown parameter. The resulting conditional probability is referred to as the *likelihood* of observing the data given the model parameters and uses the notation: \n",
    "$$L(X | \\theta)$$\n",
    "\n",
    "Calculating conditional probabilities, gven a sample comprises of n-examples, this can be framed as the joint probabilty of the observed data samples (x1, x2...xn) given the probability distribution parameters. The joint probability distribution can be restated as the multiplication of the conditional probability of observing each example, given the distribution parameters.\n",
    "\n",
    "Multiplying all the small probabilities, however, is time consuming, so we can restate this further as the sum of the log conditional probabilities of observing each example given the model parameters. This is what leads to the use of the term *log-likelihood function*.\n",
    "\n",
    "When we are *fitting* a machine learning model, we are essentially trying to estimate probability density - i.e., finding the best parameters or settings that explain the data. The Maximum Likelihood Estimation (MLE) framework is used for density estimation in both supervised and unsupervised models (and in neural nets) - it provides the basis for both Linear and Logistic Regression models. In summary, you can think of it as an optimization problem of searching a whole bunch of parameters in order to find the BEST parameters.\n",
    "\n",
    "\n",
    "### The good stuff - Logistic Regression\n",
    "\n",
    "You should use the slides to supplement your learning but this will be more detailed and probably better. Yes, you can use this for the exam. Yes, the exam will include content from this notebook (I guess you would only know that if you read this far already!!). Don't worry; I'll remind you to read it in class. Anyway, I digress...\n",
    "\n",
    "As discussed, logistic regression is used *primarily* for predicting binary classes (True/False, 0/1, Yes/No type of problems). Just like linear regression, logistic regression will have a function in the form of:\n",
    "$$ y = mx + b$$\n",
    "Where the coefficients will tell us information about how the input variables (x-values) impact our output variable (probability of Y / 1 / True). The main difference between logistic and linear regression, is that logistic regression will produce an output that is always between 0 and 1 (0 = 0% likelihood of an outcome happening; 1=100% likelihood of an outcome happening). This translates to the 0/1 binary outcome using a probability threshold - the default threshold is 50%, meaning that if the probability of an outcome is 50% or higher, it's classified as 1 (i.e., if the probability of a credit card transaction being fraudulent is calculated as 55%, it would be classified as a 1 or Yes or True).\n",
    "\n",
    "To calculate probabilities, we use the Sigmoid function (also called logistic function):\n",
    "\n",
    "$$ S(x) = {1 \\over 1+e^{-x}} $$\n",
    "\n",
    "The output is interpreted as a probability from a Binomial probability distribution function for the class labeled 0 or 1. In general when we produce the function which corresponds with the model, we are estimating the coefficients / parameters from the sample of observations that we have - in theory, these samples are representative of the population. We must always remember there will be 'noise' in the data, which is what we typically consider the errors. For logistic regression, we look at MLE to estimate parameters (if you remember, for Linear regression we discussed Least Squares Optimization - this is similar).\n",
    "\n",
    "#### Odds and log-odds\n",
    "\n",
    "Odds, if you read the slides, are often stated as wins or losses. For example, what are the odds of me drawing a Jack of Hearts from a deck of cards? We can convert the probabilility produced by the logistic regression model to \"odds of success\", defined as the probability of success divided by the probability of not success:\n",
    "$$ odds = {p \\over (1-p)}$$\n",
    "\n",
    "The logarithm is calculated as the log of the odds (hence, log-istic regression!):\n",
    "$$log odds = {log{p\\over (1-p)}}$$\n",
    "\n",
    "Quick review here on logs -- generally with log calculations we are trying to calculate an exponent which yields a value. For example \n",
    "\n",
    "$$log_28 = 3$$ \n",
    "because \n",
    "$$2^3 = 8$$\n",
    "\n",
    "When we are not provided a log base, we assume we're using e as the base, and e = 2.71\n",
    "\n",
    "Now if we put it all together (and if you remember my drawings on the board in class); we use the logit function (above) to project odds on a scale of -infinity to + infinity - this allows to project data onto a straight line (this is the blue line in the picture on Slide 10 of the notes). This gives us:\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "We then optimize the coefficients of this straight line, using gradient descent for example (more on this in a later class... maybe next semester). Then, we use the sigmoid function to convert the -infinity to +infinity values BACK to a probability of 0-1, so we can actually classify our outputs. In sum, the objective of our model is to predict/output a probability of belonging to class 1 or not.\n",
    "\n",
    "\n",
    "In the real world, you won't have to do this by hand - but it's important to understand the logic/math behind all of the models we are learning.\n",
    "\n",
    "### The Coding Part\n",
    "\n",
    "Let's get to the good stuff. We'll start with probability and odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "2.333333333333333\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# define probability of success; \n",
    "# let's say, the probability I walk into class with a \n",
    "# coffee from Sbux on Wednesday\n",
    "p = 0.7\n",
    "print(p)\n",
    "#let's convert this to odds\n",
    "odds = p / (1-p)\n",
    "print(odds)\n",
    "#let's convert back to probability\n",
    "prob = odds / (odds + 1)\n",
    "print(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "2.333333333333333\n",
      "0.8472978603872034\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "#okay, next example - log odds; start with probability again\n",
    "p = 0.7\n",
    "print(p)\n",
    "#calculate odds\n",
    "odds = p / (1-p)\n",
    "print(odds)\n",
    "#convert to log-odds\n",
    "logodds = math.log(odds)\n",
    "print(logodds)\n",
    "#convert back to probability using sigmoid function\n",
    "prob = 1/(1+math.exp(-logodds))\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start with estimating model parameters. Again, MLE is a framework we can use to estimate parameters. We are trying to maximize the conditional probability of observing the data (X) given a specific probability distribution and its parameters.\n",
    "\n",
    "For supervised learning, we can summarize the conditional probability as predicting the probability if the output, given the input:\n",
    "\n",
    "$$P(Y | X)$$\n",
    "\n",
    "To calculate maximum likelihood, we need start somewhere (i.e., make an assumption about the distribution). For logistic regression, we assume a Binomial distribution. Each example in a binomial distribution is a Bernoulli trial. Since the Bernoulli distribution has a single parameter (p), we can describe this as:\n",
    "\n",
    "$$P(y=1) = p$$\n",
    "$$P(y=0) = 1-p$$\n",
    "\n",
    "The mean of the Bernoulli distribution, therefore is:\n",
    "\n",
    "$$\\mu = P(y=1)*1 + P(y=0)*0$$\n",
    "\n",
    "THEREFORE, to calculate likelihood of a specific input, where the probability is given by our prediction function, we have:\n",
    "\n",
    "$$L = \\hat{y}*y + (1-\\hat{y})*(1-y)$$\n",
    "\n",
    "Let us test this function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "def likelihood(y,yhat):\n",
    "    return yhat * y +(1-yhat)*(1-y)\n",
    "\n",
    "y, yhat = 1,0.9\n",
    "print(likelihood(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform the likelihood into LOG-likelihood by adding the log function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10536051565782628\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = math.log(yhat)*y+math.log(1-yhat)*(1-y)\n",
    "print(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maximum likelihood, we can sum up all of the logs of all the likelihoods. We would essentially start with a baseline model, and continue to iterate until we have a model that maximizes likelihood. Generally we can use gradient descent (we will cover this in a later session) to find the optimal coefficients. Let's build a logistic regression function from scratch for Titanic, and then we'll do it with the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define a class which holds data and functions\n",
    "class LogisticRegression:\n",
    "    def __init__(self, x,y): #this is a function to initiate the class\n",
    "        self.intercept = np.ones((x.shape[0],1)) #create an array of ones same length as x variable, 1 column\n",
    "        self.x = np.concatenate((self.intercept,x),axis=1) #puts array of x values + array of ones side by side; note - x must be in array format\n",
    "        self.weight = np.zeros(self.x.shape[1]) #makes array of zeros with the same number of columns as x\n",
    "        self.y=y #output variable, also must be in array format\n",
    "\n",
    "    #sigmoid method\n",
    "    def sigmoid(self,x,weight):\n",
    "        z = np.dot(x,weight) #take the dot product of x-array, and weights \n",
    "        return 1 / (1+np.exp(-z)) #sigmoid function\n",
    "\n",
    "    #loss calculation\n",
    "    def loss(self, h, y): #y is the actual class; h is the probability of the class; this is the log loss function which we want to minimize\n",
    "        return (-y * np.log(h) - (1-y) *np.log(1-h)).mean() \n",
    "        \n",
    "    #gradient calculation\n",
    "    def gradient_descent(self, X, h, y): #gradient descent we use to minimize log function above, and find the best coefficients / straight line\n",
    "        return np.dot(X.T, (h-y)) / y.shape[0]\n",
    "        \n",
    "    #define fitting function\n",
    "    def fit(self, lr, iterations): #lr is the learning rate the model uses to determine the step size at each iteration while moving toward a minimum loss funciton - we'll cover this in gradient descent \n",
    "        for i in range(iterations): #basically we're doing a bunch of iterations; sigmoud, calculate loss, gradient descent\n",
    "            sigma = self.sigmoid(self.x,self.weight)\n",
    "            loss = self.loss(sigma,self.y)\n",
    "            dW = self.gradient_descent(self.x, sigma, self.y)\n",
    "\n",
    "            #update weights\n",
    "            self.weight -=lr*dW #then find best weights\n",
    "        return print(\"finito!\")\n",
    "        \n",
    "    #predict function\n",
    "    def predict(self,x_new,threshold): #fitting function, plus ability to set threshold\n",
    "        x_new = np.concatenate((self.intercept, x_new), axis=1) #repeat process above\n",
    "        result = self.sigmoid(x_new, self.weight)\n",
    "        result = result >= threshold #returns true/false if above threshold\n",
    "        y_pred = np.zeros(result.shape[0]) #create array of zeroes\n",
    "        for i in range(len(y_pred)):\n",
    "            if result[i] == True:\n",
    "                y_pred[i] =1 #replace 0 in array if probability is above threshold\n",
    "            else:\n",
    "                continue\n",
    "        return y_pred #return array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out on Titanic - and then we'll use the sklearn logistic regression function and also compare results with other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import load_dataset #titanic lives in seaborn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "data = load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() #success! by the way, since i'm taking this from seaborn it's slightly different than the one we used in class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info() #missing data in Age, Embarked, Class, Deck, Embark Town"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and transforming data\n",
    "\n",
    "Let's get rid of thte deck column - not useful, too many missing values. We can also remove the adult_male column, since it's redundant to sex. We can remove 'alive' since we don't care how long survivors lived, and it's redundant to survive. We can remove 'alone'. Remove Embark_town since it's redundant with Embarked. We can fill in missing values for Age and for Embarked. Also let's get rid of \"who\", names won't be relevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['alive','alone','embark_town','who','adult_male','deck','class'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "886         0       2    male  27.0      0      0  13.0000        S\n",
       "887         1       1  female  19.0      0      0  30.0000        S\n",
       "888         0       3  female   NaN      1      2  23.4500        S\n",
       "889         1       1    male  26.0      0      0  30.0000        C\n",
       "890         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill in missing data for Age, using median by sex - this is probably the safest approach. For Embarked, we can just find the most popular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarked</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>130</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "      <td>554</td>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          survived  pclass  sex  age  sibsp  parch  fare\n",
       "embarked                                                \n",
       "C              168     168  168  130    168    168   168\n",
       "Q               77      77   77   28     77     77    77\n",
       "S              644     644  644  554    644    644   644"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('embarked').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = data['age'].fillna(data.groupby('sex')['age'].transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embarked'] = data['embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   survived  891 non-null    int64  \n",
      " 1   pclass    891 non-null    int64  \n",
      " 2   sex       891 non-null    object \n",
      " 3   age       891 non-null    float64\n",
      " 4   sibsp     891 non-null    int64  \n",
      " 5   parch     891 non-null    int64  \n",
      " 6   fare      891 non-null    float64\n",
      " 7   embarked  891 non-null    object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 55.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make dummy variables for Embarked and Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_dummies = pd.get_dummies(data['sex'])\n",
    "embarked_dummies = pd.get_dummies(data['embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data,sex_dummies,embarked_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop old columns\n",
    "data.drop(['sex','embarked'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare', 'female', 'male',\n",
       "       'C', 'Q', 'S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready. Let's try the model. Remember, we should convert our variables to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[['pclass', 'age', 'sibsp', 'parch', 'fare', 'female', 'male',\n",
    "       'C', 'Q', 'S']])\n",
    "y = np.array(data.survived)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/delinaivanova/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/delinaivanova/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finito!\n",
      "accuracy -> 0.45342312008978675\n"
     ]
    }
   ],
   "source": [
    "#create the object\n",
    "regressor = LogisticRegression(x,y)\n",
    " \n",
    "#fit the model, 0.1 learning rate, anad 5000 iterations (this is how we'll find max likelihood)\n",
    "regressor.fit(0.1 , 5000)\n",
    " \n",
    " \n",
    "y_pred = regressor.predict(x,0.5) #use 0.5 treshold\n",
    " \n",
    "print('accuracy -> {}'.format(sum(y_pred == y) / y.shape[0])) #print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/delinaivanova/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/delinaivanova/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/delinaivanova/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finito!\n",
      "accuracy -> 0.7508417508417509\n"
     ]
    }
   ],
   "source": [
    "#not good! Lets try different features\n",
    "x = np.array(data[['fare', 'female','age']])\n",
    "y = np.array(data.survived)\n",
    "\n",
    "#create the object\n",
    "regressor = LogisticRegression(x,y)\n",
    " \n",
    "#fit the model, 0.3 learning rate this time, anad 5000 iterations (this is how we'll find max likelihood)\n",
    "regressor.fit(0.3, 5000)\n",
    " \n",
    " \n",
    "y_pred = regressor.predict(x,0.5) #use 0.5 treshold\n",
    " \n",
    "print('accuracy -> {}'.format(sum(y_pred == y) / y.shape[0])) #print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model worked much better - I changed the learning rate to 0.3, and also reduced the number of input variables to Fare, Female (y/n), and Age. So... hopefully this gives you a good view of how we create a logistic regression. Of course we don't need to do this in the real world, so now let's use sklearn's regression and compare the results to Naive Bayes and KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many more hyperparameters we can adjust in this version, \n",
    "# such as regularization terms (we'll cover this in a later module)\n",
    "# weights, solver type, etc. for now, we'll keep the default settings\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "log = LogisticRegression()\n",
    "nb = GaussianNB()\n",
    "knn = KNeighborsClassifier() #default neighbours is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.fit(x_train,y_train)\n",
    "nb.fit(x_train,y_train)\n",
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've spoken about classification models in the past and optimizing for the threshold (i.e., the default is 50%, but what if we wanted higher thresholds and they were more accurate?). In the example below, if I use the predict() function, I will basically get the 0/1 outputs. However, I can use a predict_proba() function so I can see the actual probabilities that the model is predicting. This will help me find an optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_log = log.predict_proba(x_test)\n",
    "yhat_nb = nb.predict_proba(x_test)\n",
    "yhat_knn = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, for each observation, we can see the probability that the observation is a 1 and the probability that it's a 0. For example, the first entry has a 73% chance of being a 0 (dead), and a 27% chance of being a 1 (alive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84255143, 0.15744857],\n",
       "       [0.81444446, 0.18555554],\n",
       "       [0.81648154, 0.18351846],\n",
       "       [0.85319025, 0.14680975],\n",
       "       [0.84390219, 0.15609781],\n",
       "       [0.84677602, 0.15322398],\n",
       "       [0.83004082, 0.16995918],\n",
       "       [0.84608001, 0.15391999],\n",
       "       [0.84900019, 0.15099981],\n",
       "       [0.84677602, 0.15322398],\n",
       "       [0.32171132, 0.67828868],\n",
       "       [0.77373515, 0.22626485],\n",
       "       [0.32827078, 0.67172922],\n",
       "       [0.8501194 , 0.1498806 ],\n",
       "       [0.30890168, 0.69109832],\n",
       "       [0.29825108, 0.70174892],\n",
       "       [0.84540856, 0.15459144],\n",
       "       [0.01107086, 0.98892914],\n",
       "       [0.8599436 , 0.1400564 ],\n",
       "       [0.79135253, 0.20864747],\n",
       "       [0.31010461, 0.68989539],\n",
       "       [0.81122333, 0.18877667],\n",
       "       [0.84847875, 0.15152125],\n",
       "       [0.85678857, 0.14321143],\n",
       "       [0.75514219, 0.24485781],\n",
       "       [0.86597747, 0.13402253],\n",
       "       [0.30885044, 0.69114956],\n",
       "       [0.16468916, 0.83531084],\n",
       "       [0.84677602, 0.15322398],\n",
       "       [0.8183384 , 0.1816616 ],\n",
       "       [0.84700977, 0.15299023],\n",
       "       [0.7225839 , 0.2774161 ],\n",
       "       [0.30549179, 0.69450821],\n",
       "       [0.80029254, 0.19970746],\n",
       "       [0.84306591, 0.15693409],\n",
       "       [0.85480709, 0.14519291],\n",
       "       [0.28215136, 0.71784864],\n",
       "       [0.84700977, 0.15299023],\n",
       "       [0.81879876, 0.18120124],\n",
       "       [0.29637019, 0.70362981],\n",
       "       [0.84703653, 0.15296347],\n",
       "       [0.86916371, 0.13083629],\n",
       "       [0.84594528, 0.15405472],\n",
       "       [0.84979711, 0.15020289],\n",
       "       [0.82831308, 0.17168692],\n",
       "       [0.84677602, 0.15322398],\n",
       "       [0.77559069, 0.22440931],\n",
       "       [0.81365686, 0.18634314],\n",
       "       [0.31301352, 0.68698648],\n",
       "       [0.84126032, 0.15873968],\n",
       "       [0.84677602, 0.15322398],\n",
       "       [0.82275964, 0.17724036],\n",
       "       [0.83837884, 0.16162116],\n",
       "       [0.84395642, 0.15604358],\n",
       "       [0.26209434, 0.73790566],\n",
       "       [0.81304484, 0.18695516],\n",
       "       [0.32222782, 0.67777218],\n",
       "       [0.117746  , 0.882254  ],\n",
       "       [0.25893942, 0.74106058],\n",
       "       [0.19680255, 0.80319745],\n",
       "       [0.30541245, 0.69458755],\n",
       "       [0.23588653, 0.76411347],\n",
       "       [0.29270827, 0.70729173],\n",
       "       [0.84620131, 0.15379869],\n",
       "       [0.7644719 , 0.2355281 ],\n",
       "       [0.70086566, 0.29913434],\n",
       "       [0.63495308, 0.36504692],\n",
       "       [0.83029742, 0.16970258],\n",
       "       [0.75747241, 0.24252759],\n",
       "       [0.1740285 , 0.8259715 ],\n",
       "       [0.8122062 , 0.1877938 ],\n",
       "       [0.82023397, 0.17976603],\n",
       "       [0.15990498, 0.84009502],\n",
       "       [0.85203062, 0.14796938],\n",
       "       [0.84454522, 0.15545478],\n",
       "       [0.8419164 , 0.1580836 ],\n",
       "       [0.81542996, 0.18457004],\n",
       "       [0.72783955, 0.27216045],\n",
       "       [0.76421842, 0.23578158],\n",
       "       [0.81859857, 0.18140143],\n",
       "       [0.26671314, 0.73328686],\n",
       "       [0.22812508, 0.77187492],\n",
       "       [0.75989814, 0.24010186],\n",
       "       [0.31648592, 0.68351408],\n",
       "       [0.21880629, 0.78119371],\n",
       "       [0.85901863, 0.14098137],\n",
       "       [0.84247341, 0.15752659],\n",
       "       [0.84700977, 0.15299023],\n",
       "       [0.84784905, 0.15215095],\n",
       "       [0.84732994, 0.15267006],\n",
       "       [0.78294457, 0.21705543],\n",
       "       [0.85761992, 0.14238008],\n",
       "       [0.81640107, 0.18359893],\n",
       "       [0.19361554, 0.80638446],\n",
       "       [0.27563716, 0.72436284],\n",
       "       [0.84546124, 0.15453876],\n",
       "       [0.31808357, 0.68191643],\n",
       "       [0.52224697, 0.47775303],\n",
       "       [0.76589919, 0.23410081],\n",
       "       [0.03330642, 0.96669358],\n",
       "       [0.28284144, 0.71715856],\n",
       "       [0.82997587, 0.17002413],\n",
       "       [0.75077561, 0.24922439],\n",
       "       [0.26821453, 0.73178547],\n",
       "       [0.78138293, 0.21861707],\n",
       "       [0.28415738, 0.71584262],\n",
       "       [0.84700977, 0.15299023],\n",
       "       [0.82755474, 0.17244526],\n",
       "       [0.32292572, 0.67707428],\n",
       "       [0.81642031, 0.18357969],\n",
       "       [0.83439212, 0.16560788],\n",
       "       [0.83520149, 0.16479851],\n",
       "       [0.84847875, 0.15152125],\n",
       "       [0.84824627, 0.15175373],\n",
       "       [0.29214603, 0.70785397],\n",
       "       [0.82902752, 0.17097248],\n",
       "       [0.29458014, 0.70541986],\n",
       "       [0.84700977, 0.15299023],\n",
       "       [0.78171992, 0.21828008],\n",
       "       [0.26030784, 0.73969216],\n",
       "       [0.09763394, 0.90236606],\n",
       "       [0.7754306 , 0.2245694 ],\n",
       "       [0.86186579, 0.13813421],\n",
       "       [0.84032904, 0.15967096],\n",
       "       [0.12526116, 0.87473884],\n",
       "       [0.81185746, 0.18814254],\n",
       "       [0.2907426 , 0.7092574 ],\n",
       "       [0.5977538 , 0.4022462 ],\n",
       "       [0.29061958, 0.70938042],\n",
       "       [0.83520149, 0.16479851],\n",
       "       [0.2850832 , 0.7149168 ],\n",
       "       [0.30953075, 0.69046925],\n",
       "       [0.84696971, 0.15303029],\n",
       "       [0.0726878 , 0.9273122 ],\n",
       "       [0.83941564, 0.16058436],\n",
       "       [0.22462077, 0.77537923],\n",
       "       [0.83969212, 0.16030788],\n",
       "       [0.21710017, 0.78289983],\n",
       "       [0.76929031, 0.23070969],\n",
       "       [0.81090387, 0.18909613],\n",
       "       [0.82639426, 0.17360574],\n",
       "       [0.84400283, 0.15599717],\n",
       "       [0.85587108, 0.14412892],\n",
       "       [0.27249131, 0.72750869],\n",
       "       [0.85992472, 0.14007528],\n",
       "       [0.84652848, 0.15347152],\n",
       "       [0.84091   , 0.15909   ],\n",
       "       [0.83995425, 0.16004575],\n",
       "       [0.83550715, 0.16449285],\n",
       "       [0.27703645, 0.72296355],\n",
       "       [0.84492897, 0.15507103],\n",
       "       [0.28239224, 0.71760776],\n",
       "       [0.84784235, 0.15215765],\n",
       "       [0.83969721, 0.16030279],\n",
       "       [0.32292572, 0.67707428],\n",
       "       [0.16079906, 0.83920094],\n",
       "       [0.81806509, 0.18193491],\n",
       "       [0.81672774, 0.18327226],\n",
       "       [0.81966787, 0.18033213],\n",
       "       [0.84410228, 0.15589772],\n",
       "       [0.83920078, 0.16079922],\n",
       "       [0.8367679 , 0.1632321 ],\n",
       "       [0.84673029, 0.15326971],\n",
       "       [0.30267629, 0.69732371],\n",
       "       [0.18199553, 0.81800447],\n",
       "       [0.32280192, 0.67719808],\n",
       "       [0.25904   , 0.74096   ],\n",
       "       [0.24765972, 0.75234028],\n",
       "       [0.85555227, 0.14444773],\n",
       "       [0.31192708, 0.68807292],\n",
       "       [0.87816057, 0.12183943],\n",
       "       [0.31095942, 0.68904058],\n",
       "       [0.84090248, 0.15909752],\n",
       "       [0.32814779, 0.67185221],\n",
       "       [0.84495515, 0.15504485],\n",
       "       [0.84784235, 0.15215765],\n",
       "       [0.84728939, 0.15271061],\n",
       "       [0.84620131, 0.15379869],\n",
       "       [0.21451986, 0.78548014]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick comparison to the predict function: we can see below that the same first observation was naturally classified as a 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on. I'm going to isolate only the probabilities that something is a 1 (alive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15744857, 0.18555554, 0.18351846, 0.14680975, 0.15609781,\n",
       "       0.15322398, 0.16995918, 0.15391999, 0.15099981, 0.15322398,\n",
       "       0.67828868, 0.22626485, 0.67172922, 0.1498806 , 0.69109832,\n",
       "       0.70174892, 0.15459144, 0.98892914, 0.1400564 , 0.20864747,\n",
       "       0.68989539, 0.18877667, 0.15152125, 0.14321143, 0.24485781,\n",
       "       0.13402253, 0.69114956, 0.83531084, 0.15322398, 0.1816616 ,\n",
       "       0.15299023, 0.2774161 , 0.69450821, 0.19970746, 0.15693409,\n",
       "       0.14519291, 0.71784864, 0.15299023, 0.18120124, 0.70362981,\n",
       "       0.15296347, 0.13083629, 0.15405472, 0.15020289, 0.17168692,\n",
       "       0.15322398, 0.22440931, 0.18634314, 0.68698648, 0.15873968,\n",
       "       0.15322398, 0.17724036, 0.16162116, 0.15604358, 0.73790566,\n",
       "       0.18695516, 0.67777218, 0.882254  , 0.74106058, 0.80319745,\n",
       "       0.69458755, 0.76411347, 0.70729173, 0.15379869, 0.2355281 ,\n",
       "       0.29913434, 0.36504692, 0.16970258, 0.24252759, 0.8259715 ,\n",
       "       0.1877938 , 0.17976603, 0.84009502, 0.14796938, 0.15545478,\n",
       "       0.1580836 , 0.18457004, 0.27216045, 0.23578158, 0.18140143,\n",
       "       0.73328686, 0.77187492, 0.24010186, 0.68351408, 0.78119371,\n",
       "       0.14098137, 0.15752659, 0.15299023, 0.15215095, 0.15267006,\n",
       "       0.21705543, 0.14238008, 0.18359893, 0.80638446, 0.72436284,\n",
       "       0.15453876, 0.68191643, 0.47775303, 0.23410081, 0.96669358,\n",
       "       0.71715856, 0.17002413, 0.24922439, 0.73178547, 0.21861707,\n",
       "       0.71584262, 0.15299023, 0.17244526, 0.67707428, 0.18357969,\n",
       "       0.16560788, 0.16479851, 0.15152125, 0.15175373, 0.70785397,\n",
       "       0.17097248, 0.70541986, 0.15299023, 0.21828008, 0.73969216,\n",
       "       0.90236606, 0.2245694 , 0.13813421, 0.15967096, 0.87473884,\n",
       "       0.18814254, 0.7092574 , 0.4022462 , 0.70938042, 0.16479851,\n",
       "       0.7149168 , 0.69046925, 0.15303029, 0.9273122 , 0.16058436,\n",
       "       0.77537923, 0.16030788, 0.78289983, 0.23070969, 0.18909613,\n",
       "       0.17360574, 0.15599717, 0.14412892, 0.72750869, 0.14007528,\n",
       "       0.15347152, 0.15909   , 0.16004575, 0.16449285, 0.72296355,\n",
       "       0.15507103, 0.71760776, 0.15215765, 0.16030279, 0.67707428,\n",
       "       0.83920094, 0.18193491, 0.18327226, 0.18033213, 0.15589772,\n",
       "       0.16079922, 0.1632321 , 0.15326971, 0.69732371, 0.81800447,\n",
       "       0.67719808, 0.74096   , 0.75234028, 0.14444773, 0.68807292,\n",
       "       0.12183943, 0.68904058, 0.15909752, 0.67185221, 0.15504485,\n",
       "       0.15215765, 0.15271061, 0.15379869, 0.78548014])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_log[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the roc_curve() function to plot these. A True Positive will be anything over 50%, because this would be classified as a 1. A False Positive will be anything where the prediction was over 50% but actually the output should have been a 0. You can print out each element to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, yhat_log[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.00877193, 0.00877193,\n",
       "       0.01754386, 0.01754386, 0.02631579, 0.02631579, 0.03508772,\n",
       "       0.03508772, 0.04385965, 0.04385965, 0.05263158, 0.05263158,\n",
       "       0.06140351, 0.06140351, 0.07017544, 0.07017544, 0.07894737,\n",
       "       0.07894737, 0.0877193 , 0.0877193 , 0.11403509, 0.11403509,\n",
       "       0.13157895, 0.13157895, 0.14035088, 0.14035088, 0.14912281,\n",
       "       0.14912281, 0.15789474, 0.15789474, 0.15789474, 0.23684211,\n",
       "       0.23684211, 0.26315789, 0.26315789, 0.28947368, 0.28947368,\n",
       "       0.29824561, 0.29824561, 0.31578947, 0.31578947, 0.40350877,\n",
       "       0.40350877, 0.42105263, 0.42105263, 0.43859649, 0.43859649,\n",
       "       0.49122807, 0.50877193, 0.5877193 , 0.5877193 , 0.61403509,\n",
       "       0.61403509, 0.63157895, 0.63157895, 0.64035088, 0.64035088,\n",
       "       0.66666667, 0.66666667, 0.69298246, 0.71052632, 0.71929825,\n",
       "       0.71929825, 0.76315789, 0.76315789, 0.79824561, 0.8245614 ,\n",
       "       0.84210526, 0.85087719, 0.85087719, 0.85964912, 0.87719298,\n",
       "       0.87719298, 1.        ])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.01538462, 0.15384615, 0.15384615, 0.2       ,\n",
       "       0.2       , 0.27692308, 0.27692308, 0.29230769, 0.29230769,\n",
       "       0.36923077, 0.36923077, 0.38461538, 0.38461538, 0.4       ,\n",
       "       0.4       , 0.43076923, 0.43076923, 0.44615385, 0.44615385,\n",
       "       0.46153846, 0.46153846, 0.49230769, 0.49230769, 0.50769231,\n",
       "       0.50769231, 0.52307692, 0.52307692, 0.53846154, 0.53846154,\n",
       "       0.56923077, 0.56923077, 0.58461538, 0.61538462, 0.61538462,\n",
       "       0.64615385, 0.64615385, 0.69230769, 0.69230769, 0.70769231,\n",
       "       0.70769231, 0.72307692, 0.72307692, 0.73846154, 0.73846154,\n",
       "       0.75384615, 0.75384615, 0.8       , 0.8       , 0.81538462,\n",
       "       0.81538462, 0.81538462, 0.81538462, 0.83076923, 0.83076923,\n",
       "       0.84615385, 0.84615385, 0.86153846, 0.86153846, 0.87692308,\n",
       "       0.87692308, 0.90769231, 0.90769231, 0.90769231, 0.90769231,\n",
       "       0.92307692, 0.92307692, 0.93846154, 0.95384615, 0.95384615,\n",
       "       0.95384615, 0.95384615, 0.96923077, 0.98461538, 0.98461538,\n",
       "       1.        , 1.        ])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.98892914, 0.98892914, 0.8259715 , 0.81800447, 0.78548014,\n",
       "       0.78289983, 0.75234028, 0.74106058, 0.74096   , 0.73969216,\n",
       "       0.72436284, 0.72296355, 0.71784864, 0.71760776, 0.71715856,\n",
       "       0.71584262, 0.70938042, 0.7092574 , 0.70785397, 0.70729173,\n",
       "       0.70541986, 0.70362981, 0.69732371, 0.69114956, 0.69109832,\n",
       "       0.68989539, 0.68904058, 0.68807292, 0.68698648, 0.68351408,\n",
       "       0.67828868, 0.67777218, 0.67719808, 0.67707428, 0.24922439,\n",
       "       0.24252759, 0.2355281 , 0.22626485, 0.21861707, 0.21828008,\n",
       "       0.21705543, 0.20864747, 0.18909613, 0.18877667, 0.18327226,\n",
       "       0.18193491, 0.18140143, 0.17976603, 0.17360574, 0.17244526,\n",
       "       0.16560788, 0.16479851, 0.15967096, 0.15909752, 0.1580836 ,\n",
       "       0.15752659, 0.15693409, 0.15609781, 0.15604358, 0.15599717,\n",
       "       0.15507103, 0.15459144, 0.15391999, 0.15379869, 0.15347152,\n",
       "       0.15326971, 0.15322398, 0.15303029, 0.15299023, 0.15267006,\n",
       "       0.15215765, 0.15215095, 0.15175373, 0.15152125, 0.15020289,\n",
       "       0.1498806 , 0.12183943])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we want this curve to be towards the top left; but in a non-ideal world, we want to find the optimal threshold. One way we can do this is by calculating the geometric mean (G-mean) which will find the balance between Sensitivity and Specificity. As a refresher:\n",
    "\n",
    "Sensitivity = True Positive Rate \\\n",
    "Specificity = 1 - False Positive Rate\n",
    "\n",
    "In other words:\n",
    "\n",
    "Sensitivity = TP / (TP + FN) \\\n",
    "Specificity = TN / (FP + TN)\n",
    "\n",
    "To calculate the G-mean, we simply take the square root of the Sensitivity multiplied by the Specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.12403473, 0.39223227, 0.39050816, 0.44524781,\n",
       "       0.44327331, 0.52159829, 0.51926451, 0.53349357, 0.531085  ,\n",
       "       0.59688802, 0.59416869, 0.60642088, 0.60363273, 0.61558701,\n",
       "       0.61273044, 0.63586043, 0.63288215, 0.64408447, 0.64103914,\n",
       "       0.65199786, 0.64888568, 0.67016625, 0.66042966, 0.67066949,\n",
       "       0.663996  , 0.67398146, 0.67056888, 0.68035872, 0.6768786 ,\n",
       "       0.69594934, 0.69235268, 0.70164642, 0.71987403, 0.68529966,\n",
       "       0.70222319, 0.69000968, 0.71422788, 0.70135785, 0.7091079 ,\n",
       "       0.70471709, 0.71233588, 0.70337532, 0.71081865, 0.66369107,\n",
       "       0.67056888, 0.66063397, 0.68055705, 0.67016625, 0.67657947,\n",
       "       0.64408447, 0.63288215, 0.5797994 , 0.58524364, 0.56625769,\n",
       "       0.57147677, 0.55833761, 0.56339055, 0.55664311, 0.56159115,\n",
       "       0.54065487, 0.55005827, 0.5278991 , 0.51259442, 0.50476809,\n",
       "       0.50902781, 0.4675719 , 0.47145223, 0.43868285, 0.40907387,\n",
       "       0.38808155, 0.37714747, 0.38017682, 0.37174135, 0.34773219,\n",
       "       0.35043832, 0.        ])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmeans = np.sqrt(tpr*(1-fpr))\n",
    "gmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6770742760093862\n",
      "0.7198740330653561\n"
     ]
    }
   ],
   "source": [
    "#find the index with the highest gmean\n",
    "\n",
    "print(thresholds[np.argmax(gmeans)])\n",
    "print(gmeans[np.argmax(gmeans)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for this model, we can conclude that the best threshold is 61%, with a geometric mean of 81%. So how do we actually implement this?\n",
    "\n",
    "Unfortunately, sklearn models don't have a threshold parameter as a hyperparameter. But, we can simply use the predict_proba() function, then assign a class based on the optimal threshold, and go from there. \n",
    "\n",
    "Now let's repeat the process for NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09525991655466014\n",
      "0.7254762501100117\n"
     ]
    }
   ],
   "source": [
    "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, yhat_nb[:,1])\n",
    "gmeans_nb = np.sqrt(tpr_nb*(1-fpr_nb))\n",
    "print(thresholds_nb[np.argmax(gmeans_nb)])\n",
    "print(gmeans_nb[np.argmax(gmeans_nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.12403473, 0.1754116 , 0.17464056, 0.34928112,\n",
       "       0.34773219, 0.40775214, 0.40592774, 0.48956727, 0.48735702,\n",
       "       0.54488172, 0.54239933, 0.56887321, 0.56363003, 0.57629746,\n",
       "       0.57359816, 0.58593501, 0.57758397, 0.58949417, 0.58084586,\n",
       "       0.61470948, 0.61162821, 0.62245433, 0.61930264, 0.64030189,\n",
       "       0.65054737, 0.64387491, 0.66369107, 0.66022529, 0.67936622,\n",
       "       0.67217696, 0.70853673, 0.68922691, 0.69778907, 0.68981407,\n",
       "       0.69817576, 0.68174579, 0.68981407, 0.68559499, 0.6935212 ,\n",
       "       0.68922691, 0.71987403, 0.71536067, 0.72277393, 0.71818485,\n",
       "       0.72547625, 0.71138799, 0.71846665, 0.67906819, 0.6856934 ,\n",
       "       0.67016625, 0.67657947, 0.58593501, 0.59143687, 0.56625769,\n",
       "       0.55323853, 0.53990552, 0.54981287, 0.54289671, 0.56195149,\n",
       "       0.53235403, 0.51692019, 0.50101112, 0.47145223, 0.47530088,\n",
       "       0.46641597, 0.41917598, 0.35426836, 0.35993702, 0.22764416,\n",
       "       0.20942695, 0.13245324, 0.        ])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmeans_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.00877193, 0.00877193,\n",
       "       0.01754386, 0.01754386, 0.02631579, 0.02631579, 0.03508772,\n",
       "       0.03508772, 0.04385965, 0.04385965, 0.06140351, 0.06140351,\n",
       "       0.07017544, 0.07017544, 0.09649123, 0.09649123, 0.12280702,\n",
       "       0.12280702, 0.13157895, 0.13157895, 0.14035088, 0.14035088,\n",
       "       0.14035088, 0.15789474, 0.15789474, 0.16666667, 0.16666667,\n",
       "       0.18421053, 0.18421053, 0.22807018, 0.22807018, 0.24561404,\n",
       "       0.24561404, 0.28070175, 0.28070175, 0.28947368, 0.28947368,\n",
       "       0.29824561, 0.29824561, 0.30701754, 0.30701754, 0.31578947,\n",
       "       0.31578947, 0.34210526, 0.34210526, 0.4122807 , 0.4122807 ,\n",
       "       0.43859649, 0.43859649, 0.57894737, 0.57894737, 0.61403509,\n",
       "       0.63157895, 0.64912281, 0.64912281, 0.65789474, 0.65789474,\n",
       "       0.69298246, 0.71052632, 0.72807018, 0.76315789, 0.76315789,\n",
       "       0.77192982, 0.81578947, 0.86842105, 0.86842105, 0.94736842,\n",
       "       0.95614035, 0.98245614, 1.        ])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeZUlEQVR4nO3deZgdZZ328e9Np7ORFbIQkjQJkEAia2jABTAsYsAoICgIo6ODRgTEeV1eGBmdGdxQeH0FAZmIDKIIoqIii4jjsAgiCRBCQghEMCSQQMCQhOzd/Zs/qhIPTS+nk1On+nTdn+vqq09VPafqrk6u8zu1PY8iAjMzK64d8g5gZmb5ciEwMys4FwIzs4JzITAzKzgXAjOzguuVd4CuGjZsWIwbNy7vGGZmNeWRRx55JSKGt7Ws5grBuHHjmD17dt4xzMxqiqTF7S3zqSEzs4JzITAzKzgXAjOzgnMhMDMrOBcCM7OCy6wQSLpW0suS5rWzXJIul7RI0lxJU7LKYmZm7cvyiOA6YFoHy48DJqQ/M4DvZZjFzMzakdlzBBFxn6RxHTQ5Abg+kn6wH5I0RNKoiFiWVSYzs7ytWreZGx5ezIZNzV1+b+O4nThiYpvPhG2XPB8oGw0sKZlems57UyGQNIPkqIGGhoaqhDMzq7QNm5s584ezmL14JVLX33/WO/focYWgrT9Dm6PkRMRMYCZAY2OjR9Ixs5rT0hJ87ubHmb14JVecfiDT99s170hb5VkIlgJjS6bHAC/mlMXMCu6OJ5ax+NV1ma3/yWWruf2JZVx4/KRuVQQg30JwK3CupJuAQ4FVvj5gZnlobgnO/cmjtGR8vuETh4/n44ePz3Yj2yCzQiDpRmAqMEzSUuDfgHqAiLgauAM4HlgErAM+llUWM7OORAQtAecdPYGzp+6R2Xb61tdltu7tkeVdQx/qZHkA52S1fTOzrqrfQd32wzpLfrLYzKzgXAjMzAqu5gamMbOe6f5nVrDstQ25bLs5in1XuguBmeVu/aZmPnLtw+T9eTxsYJ98A+TEhcDMctfU0kIEfPqoPTntkHx6D6iTGDnIhcDMLBfr0n53BverZ/SQfjmnKR4XAjOruojgqeVruGfhCu5Z+DKPLF4JJIXAqs+FwMyqYvWGzfzxmVe4d+EK7n16BctXJxeGJ40axCeO2J2pE4dzyPidck5ZTC4EZgbA40teY8WajRVdZwBPv7SGexeu4JHnV9LcEgzs24vDJwxj6sQRvHOv4Ywc1Lei27SucyEwM1at38yJVz2Q2V07k0cN4pNH7M7UvUYwpWEIver8CFN34kJgZmxsaiYCPjV1D47fZ1RF1z1ycB9GDPS3/u7MhcDMtho9pB/7jhmcdwyrMh+fmZkVnAuBmVnBuRCYmRWcC4GZWcG5EJiZFZwLgZlZwbkQmJkVnAuBmVnB+YEyswLb2NTM8lUbeHXtpryjWI5cCMwK7LwbH+Ou+S9tne7dyycJisiFwKzA/rZ2E3uOGMDZU/egvm4Hjpk0Mu9IlgMXArOCGzGwD++fMibvGJYjHweamRWcC4GZWcH51JBZjdjY1MymppaKrrOpJaivq+gqrQa5EJjVgNUbNvOOb/yBNRubKr7uIyYOr/g6rba4EJjVgNXrN7NmYxPT9xvFAWOHVHTdb99jWEXXZ7XHhcAKLSJoyWic3kpqSc8IHTFxOB9sHJtvGOtxXAis0Gb86BHufvKlzht2E3VS3hGsB8q0EEiaBlwG1AHXRMTFrZYPBn4MNKRZLo2I/8oyk1mpZ1e8zsSRA5i+3655R+lU7147cMxkP/BllZdZIZBUB1wJvAtYCsySdGtEPFnS7BzgyYh4r6ThwEJJN0SEOz6xqpkwciDnHT0h7xhmucnyiOAQYFFEPAsg6SbgBKC0EAQwUJKAAcDfgMrfFmGFdc4Nj3Z46mdTcwuTdx1cxURm3U+WhWA0sKRkeilwaKs2VwC3Ai8CA4FTI+JNN0pLmgHMAGhoaMgkrPVMC5atpmHn/ryrg1Mq79l3VBUTmXU/WRaCtq5qtb4/493AHOAoYA/gbkn3R8TqN7wpYiYwE6CxsbEG7vGw7mTSqEGcP23vvGOYdVtZFoKlQOl9bmNIvvmX+hhwcUQEsEjSc8DewMMZ5rJuaMWajZx01QOs2VDZM4OrN2xmn9E+9WPWkSwLwSxggqTxwAvAacDprdo8DxwN3C9pJLAX8GyGmaybevG19SxduZ6j9x7B2J36V3Td7zug+98RZJanzApBRDRJOhe4i+T20WsjYr6ks9LlVwNfAa6T9ATJqaTzI+KVrDJZ93fGWxs4am/fImlWTZk+RxARdwB3tJp3dcnrF4Fjs8xg3cOKNRs5/fsPtXvqZ3Nzco+A2ry0ZGZZ8pPFVhVLVq7jmZdf5/AJw9h1cL822/TrXceU3YZWOZmZuRBYVZ152Him7jUi7xhmVsKFwDJ14S+f4E9/eZUNm5vzjmJm7XAhsEz94amX6VUnDhq3E4fX13HgWJ/6MetuXAgsc2/bfWe+dcr+eccws3a4EFjF/eGpl/h/v3uaiORuITPr3jx4vVXcH595laeWr2HXIf04cu8RvG//0XlHMrMO+IjAMtG/vo5r/rEx7xhmVgYfEZiZFZwLgZlZwZVdCCTtmGUQMzPLR6eFQNLbJT0JLEin95d0VebJzMysKso5Ivj/JAPIvAoQEY8DR2QZyszMqqesU0MRsaTVLPcXYGbWQ5Rz++gSSW8HQlJv4DzS00RmZlb7yjkiOAs4h2Qw+qXAAcDZGWYyM7MqKueIYK+IOKN0hqR3AA9kE8nMzKqpnELwXWBKGfOswNZtauJrty/g9Y1NPPHCqrzjmFkXtFsIJL0NeDswXNJnSxYNIhmD2GyrBctWc8Ofn2fEwD70713HUZM8+IxZrejoiKA3MCBtM7Bk/mrglCxDWe269AP7c8TE4XnHMLMuaLcQRMS9wL2SrouIxVXMZDXk5llLeGzJSnc3bVbDyrlGsE7SJcBbgL5bZkbEUZmlsprx7buf5rX1mxjUt57xw3Zk/DD3RGJWa8opBDcAPwWmk9xK+o/AiixDWe0IghMPGM3FJ++XdxQz20blFIKdI+IHkj5Tcrro3qyDWX6eWr6am2ctJYhO265e31SFRGaWpXIKweb09zJJ7wFeBMZkF8nydtPDS7juwb8ysG/n/z3q68Q+owdXIZWZZaWcQvBVSYOBz5E8PzAI+OcsQ1m+IoIh/euZ8+Vj845iZlXQaSGIiNvSl6uAI2Hrk8XWA9z5xDLmvfjGB8Aeff61fMKYWS46eqCsDvggSR9Dv42IeZKmA18E+gEHVieiZelLv57Pq2s3Uie9Yf7B43bKKZGZVVtHRwQ/AMYCDwOXS1oMvA24ICJ+VYVsVgURwRmHNvDVE/fNO4qZ5aSjQtAI7BcRLZL6Aq8Ae0bE8upEs67Y2NTMT/78POs2dW2oiK62N7Oep6NCsCkiWgAiYoOkp7taBCRNAy4j6Zvomoi4uI02U4HvAPXAKxHxzq5swxKPLF7Jf/zmyW1677id/RCYWZF1VAj2ljQ3fS1gj3RaQEREh08QpdcYrgTeRTKOwSxJt0bEkyVthgBXAdMi4nlJ7qlsG7W0JL9/8olDadyta+f3e/cqa6A6M+uhOioEk7Zz3YcAiyLiWQBJNwEnAKVfW08HbomI5wEi4uXt3Gbh1dft4A92M+uSjjqd296O5kYDpWMdLwUObdVmIlAv6R6SHk4vi4jrW69I0gxgBkBDQ8N2xjIzs1JZfnVUG/Na91nQCzgIeA/wbuBLkia+6U0RMyOiMSIahw93F8dmZpVUzpPF22opye2nW4wh6Z6idZtXImItsFbSfcD+wNMZ5jIzsxJlHRFI6idpry6uexYwQdJ4Sb2B04BbW7X5NXC4pF6S+pOcOlrQxe2Ymdl26LQQSHovMAf4bTp9gKTWH+hvEhFNwLnAXSQf7jdHxHxJZ0k6K22zIF3vXJIH166JiHnbuC9mZrYNyjk19O8kdwDdAxARcySNK2flEXEHcEereVe3mr4EuKSc9ZmZWeWVc2qoKSJWdd7MzMxqUTlHBPMknQ7USZoAnAc8mG0sMzOrlnIKwaeBC4GNwE9Izvl/NctQ1rHX1m3i4ef+9oZ7cRcsW51bHjOrbeUUgr0i4kKSYmDdwHd+/wzXPfjXNpcN6ltf3TBmVvPKKQTfljQK+BlwU0TMzziTdWLD5mZ22rE3PzrzkDfMH9innoad++eUysxqVTkjlB0paReSQWpmShoE/DQifHooR/V14i27eqxgM9t+ZT1QFhHLI+Jy4CySZwq+nGUoMzOrnnIeKJsk6d8lzQOuILljaEzmyczMrCrKuUbwX8CNwLER0bqvIKuSiODJZatZv6mZFWs25h3HzHqQcq4RvLUaQaxjjy15jfdf9ffHN3Yf5lHFzKwy2i0Ekm6OiA9KeoI3dh9d1ghlVlmvb2gC4EvTJzNx5AAPL2lmFdPREcFn0t/TqxHEynPA2MEc1MWhKM3MOtLuxeKIWJa+PDsiFpf+AGdXJ56ZmWWtnNtH39XGvOMqHcTMzPLR0TWCT5F8899d0tySRQOBB7IOVmTNLcHiV9e+Yd7yVRtySmNmPV1H1wh+AtwJfAO4oGT+moj4W6apCu5bdz3Ff977bJvL+vSqq3IaM+vpOioEERF/lXRO6wWSdnIxyM7KtZsY3K+ei054yxvmD+jTi8mjBuWUysx6qs6OCKYDj5DcPqqSZQHsnmGuwuvfu44TDhiddwwzK4B2C0FETE9/j69eHDMzq7Zy+hp6h6Qd09f/IOnbkhqyj2ZmZtVQzu2j3wPWSdof+L/AYuBHmaYyM7OqKXfw+gBOAC6LiMtIbiE1M7MeoJzeR9dI+hfgw8DhkuoAj4doZtZDlHNEcCrJwPX/FBHLgdHAJZmmKqim5hY2NjXT3JJ3EjMrknK6oV4u6QbgYEnTgYcj4vrsoxXLwuVreO8Vf2RTU1IFxgztl3MiMyuKTguBpA+SHAHcQ/IswXclfSEifp5xtkJ5cdV6NjW1cMahDew6pB/7jvZ4xGZWHeVcI7gQODgiXgaQNBz4PeBCkIFTDhrDgQ1D845hZgVSTiHYYUsRSL1KmYPeW+dOuuoB5i5dRUskY//sIHXyDjOzyiqnEPxW0l0k4xZDcvH4juwiFcvC5WvYd/RgDttzGAP79mLyru5LyMyqq5yLxV+Q9H7gMJJrBDMj4peZJyuQg8cN5fPv3ivvGGZWUB2NRzABuBTYA3gC+HxEvFCtYGZmVh0dneu/FrgNOJmkB9LvdnXlkqZJWihpkaQLOmh3sKRmSad0dRtmZrZ9Ojo1NDAivp++Xijp0a6sOH0C+UqSoS6XArMk3RoRT7bR7pvAXV1Zv5mZVUZHhaCvpAP5+zgE/UqnI6KzwnAIsCgingWQdBNJf0VPtmr3aeAXwMFdzG5mZhXQUSFYBny7ZHp5yXQAR3Wy7tHAkpLppcChpQ0kjQZOStfVbiGQNAOYAdDQ4B6wzcwqqaOBaY7cznW3dUN8tJr+DnB+RDSrg/vnI2ImMBOgsbGx9TrMzGw7lPMcwbZaCowtmR4DvNiqTSNwU1oEhgHHS2qKiF9lmKuqzr7hEea9sLrd5es2NVcxjZnZm2VZCGYBEySNB14ATgNOL21QOgympOuA23pSEQD47wUv07BTf/Zpp++gxnFDPTaxmeUqs0IQEU2SziW5G6gOuDYi5ks6K11+dVbb7m6OmjSCfzluUt4xzMzaVE7vowLOAHaPiIvS8Yp3iYiHO3tvRNxBq+4o2isAEfHRshJ3M//np3NYsKz9Uz8bmzy4gJl1b+UcEVwFtJDc2XMRsAbf7rnV7XOXMXpoPyaOHNDm8vHDduS4fUZVOZWZWfnKKQSHRsQUSY8BRMRKSb0zzlVTpu2zC+dP2zvvGGZm26ScQrA5ffo3YOt4BIU+3/Gbx1/kB398DoBNHlfSzGpcOeMKXA78Ehgh6WvAH4GvZ5qqm/v9gpd4avlqBvWrZ+pewzlm0oi8I5mZbbNyuqG+QdIjwNEkD4mdGBELMk/Wze0yqC/X/9MheccwM9tu5dw11ACsA35TOi8ins8yWHfxH7+Zz8Lla94w7+mX1jCgT5aPYJiZVU85n2a3k1wfENAXGA8sBN6SYa5u4/o/LWbEwD6MGdpv67zxw3bk8AnDc0xlZlY55Zwa2rd0WtIU4JOZJeqGTp4yxiOImVmP1eVB6NPup/0MgZlZD1HONYLPlkzuAEwBVmSWyMzMqqqcawQDS143kVwz+EU2cczMrNo6LATpg2QDIuILVcrTLdz/zAp+OisZU6e5xcMfmFnP1m4hkNQr7UF0SjUDdQc3z17KXfOXM3an/uw5YgAHjRuadyQzs8x0dETwMMn1gDmSbgV+BqzdsjAibsk4W67GDu3PHz43Ne8YZmaZK+cawU7AqyS9j255niCAHl0IzMyKoqNCMCK9Y2gefy8AW/jEuZlZD9FRIagDBlDeIPRmZlajOioEyyLioqolMTOzXHT0ZHFbRwJmZtbDdFQIjq5aCjMzy027hSAi/lbNIGZmlo8udzpnZmY9iwuBmVnBeZit1NqNTfz4ocVs2NzC061GJDMz68lcCFIPLHqFb9z51NbpYyaNzDGNmVn1uBCkWiJ5Ru728w5j0i6DkG+eNbOCcCFoZQeJHXZwFTCz4vDFYjOzgnMhMDMrOBcCM7OCy7QQSJomaaGkRZIuaGP5GZLmpj8PSto/yzxmZvZmmRWCdLzjK4HjgMnAhyRNbtXsOeCdEbEf8BVgZlZ5zMysbVkeERwCLIqIZyNiE3ATcEJpg4h4MCJWppMPAWMyzGNmZm3IshCMBpaUTC9N57XnTODOthZImiFptqTZK1asqGBEMzPLshCUPbKZpCNJCsH5bS2PiJkR0RgRjcOHD69gRDMzy/KBsqXA2JLpMcCLrRtJ2g+4BjguIl7NMI+ZmbUhyyOCWcAESeMl9QZOA24tbSCpAbgF+HBEPJ1hFjMza0dmRwQR0STpXOAuoA64NiLmSzorXX418GVgZ+AqJZ37NEVEY1aZzMzszTLtaygi7gDuaDXv6pLXHwc+nmWGzjzz0hqWvraeeS+szjOGmVluCt/p3IlXPsDaTc1bpwf0KfyfxMwKpvCfeus2N/OBg8Zw+qENDO5Xz9id+ucdycysqgpfCABGDe7LgQ1D845hZpYLdzpnZlZwLgRmZgXnQmBmVnAuBGZmBedCYGZWcIW8a2hTUwuLX10LQLTZDZ6ZWXEUshBcdNt8fvzQ81un+9TX5ZjGzCxfhSwEK9dtZuSgPnxp+mTqJA6bMCzvSGZmuSlkIYCkK4np++2adwwzs9z5YrGZWcG5EJiZFZwLgZlZwbkQmJkVnAuBmVnBuRCYmRWcC4GZWcG5EJiZFZwLgZlZwbkQmJkVnAuBmVnBuRCYmRWcC4GZWcG5EJiZFZwLgZlZwRVmPIKnlq/mzOtms2FzM2s2NNGwc/+8I5mZdQuFKQR/eXktL7y2nun7jWJI/3retrtHJTMzgwIVgi3OO3oCE0cOzDuGmVm34WsEZmYFl2khkDRN0kJJiyRd0MZySbo8XT5X0pQs85iZ2ZtlVggk1QFXAscBk4EPSZrcqtlxwIT0ZwbwvazymJlZ27I8IjgEWBQRz0bEJuAm4IRWbU4Aro/EQ8AQSaMyzGRmZq1kWQhGA0tKppem87raBkkzJM2WNHvFihXbFGaXwX05ft9dGNCncNfHzcw6lOWnotqYF9vQhoiYCcwEaGxsfNPychy021AO2u2gbXmrmVmPluURwVJgbMn0GODFbWhjZmYZyrIQzAImSBovqTdwGnBrqza3Ah9J7x56K7AqIpZlmMnMzFrJ7NRQRDRJOhe4C6gDro2I+ZLOSpdfDdwBHA8sAtYBH8sqj5mZtS3TK6cRcQfJh33pvKtLXgdwTpYZzMysY36y2Mys4FwIzMwKzoXAzKzgXAjMzApOyfXa2iFpBbB4G98+DHilgnFqgfe5GLzPxbA9+7xbRAxva0HNFYLtIWl2RDTmnaOavM/F4H0uhqz22aeGzMwKzoXAzKzgilYIZuYdIAfe52LwPhdDJvtcqGsEZmb2ZkU7IjAzs1ZcCMzMCq5HFgJJ0yQtlLRI0gVtLJeky9PlcyVNySNnJZWxz2ek+zpX0oOS9s8jZyV1ts8l7Q6W1CzplGrmy0I5+yxpqqQ5kuZLurfaGSutjP/bgyX9RtLj6T7XdC/Gkq6V9LKkee0sr/znV0T0qB+SLq//AuwO9AYeBya3anM8cCfJCGlvBf6cd+4q7PPbgaHp6+OKsM8l7f5A0gvuKXnnrsK/8xDgSaAhnR6Rd+4q7PMXgW+mr4cDfwN65519O/b5CGAKMK+d5RX//OqJRwSHAIsi4tmI2ATcBJzQqs0JwPWReAgYImlUtYNWUKf7HBEPRsTKdPIhktHgalk5/84AnwZ+AbxczXAZKWefTwduiYjnASKi1ve7nH0OYKAkAQNICkFTdWNWTkTcR7IP7an451dPLASjgSUl00vTeV1tU0u6uj9nknyjqGWd7rOk0cBJwNX0DOX8O08Ehkq6R9Ijkj5StXTZKGefrwAmkQxz+wTwmYhoqU68XFT88yvTgWlyojbmtb5Htpw2taTs/ZF0JEkhOCzTRNkrZ5+/A5wfEc3Jl8WaV84+9wIOAo4G+gF/kvRQRDyddbiMlLPP7wbmAEcBewB3S7o/IlZnnC0vFf/86omFYCkwtmR6DMk3ha62qSVl7Y+k/YBrgOMi4tUqZctKOfvcCNyUFoFhwPGSmiLiV1VJWHnl/t9+JSLWAmsl3QfsD9RqIShnnz8GXBzJCfRFkp4D9gYerk7Eqqv451dPPDU0C5ggabyk3sBpwK2t2twKfCS9+v5WYFVELKt20ArqdJ8lNQC3AB+u4W+HpTrd54gYHxHjImIc8HPg7BouAlDe/+1fA4dL6iWpP3AosKDKOSupnH1+nuQICEkjgb2AZ6uasroq/vnV444IIqJJ0rnAXSR3HFwbEfMlnZUuv5rkDpLjgUXAOpJvFDWrzH3+MrAzcFX6DbkparjnxjL3uUcpZ58jYoGk3wJzgRbgmoho8zbEWlDmv/NXgOskPUFy2uT8iKjZ7qkl3QhMBYZJWgr8G1AP2X1+uYsJM7OC64mnhszMrAtcCMzMCs6FwMys4FwIzMwKzoXAzKzgXAisW0p7C51T8jOug7avV2B710l6Lt3Wo5Letg3ruEbS5PT1F1ste3B7M6br2fJ3mZf2uDmkk/YHSDq+Etu2nsu3j1q3JOn1iBhQ6bYdrOM64LaI+LmkY4FLI2K/7VjfdmfqbL2Sfgg8HRFf66D9R4HGiDi30lms5/ARgdUESQMk/Xf6bf0JSW/qaVTSKEn3lXxjPjydf6ykP6Xv/Zmkzj6g7wP2TN/72XRd8yT9czpvR0m3p/3fz5N0ajr/HkmNki4G+qU5bkiXvZ7+/mnpN/T0SORkSXWSLpE0S0kf858s48/yJ9LOxiQdomScicfS33ulT+JeBJyaZjk1zX5tup3H2vo7WgHl3fe2f/zT1g/QTNKR2BzglyRPwQ9Klw0jeapyyxHt6+nvzwEXpq/rgIFp2/uAHdP55wNfbmN715GOVwB8APgzSedtTwA7knRvPB84EDgZ+H7Jewenv+8h+fa9NVNJmy0ZTwJ+mL7uTdKLZD9gBvCv6fw+wGxgfBs5Xy/Zv58B09LpQUCv9PUxwC/S1x8Frih5/9eBf0hfDyHpg2jHvP+9/ZPvT4/rYsJ6jPURccCWCUn1wNclHUHSdcJoYCSwvOQ9s4Br07a/iog5kt4JTAYeSLvW6E3yTbotl0j6V2AFSQ+tRwO/jKQDNyTdAhwO/Ba4VNI3SU4n3d+F/boTuFxSH2AacF9ErE9PR+2nv4+iNhiYADzX6v39JM0BxgGPAHeXtP+hpAkkPVHWt7P9Y4H3Sfp8Ot0XaKC2+yOy7eRCYLXiDJLRpw6KiM2S/kryIbZVRNyXFor3AD+SdAmwErg7Ij5Uxja+EBE/3zIh6Zi2GkXE05IOIunv5RuSfhcRF5WzExGxQdI9JF0nnwrcuGVzwKcj4q5OVrE+Ig6QNBi4DTgHuJykv53/iYiT0gvr97TzfgEnR8TCcvJaMfgagdWKwcDLaRE4EtitdQNJu6Vtvg/8gGS4v4eAd0jacs6/v6SJZW7zPuDE9D07kpzWuV/SrsC6iPgxcGm6ndY2p0cmbbmJpKOww0k6UyP9/akt75E0Md1mmyJiFXAe8Pn0PYOBF9LFHy1puobkFNkWdwGfVnp4JOnA9rZhxeFCYLXiBqBR0mySo4On2mgzFZgj6TGS8/iXRcQKkg/GGyXNJSkMe5ezwYh4lOTawcMk1wyuiYjHgH2Bh9NTNBcCX23j7TOBuVsuFrfyO5JxaX8fyfCLkIwT8STwqJJBy/+TTo7Y0yyPk3TN/C2So5MHSK4fbPE/wOQtF4tJjhzq02zz0mkrON8+amZWcD4iMDMrOBcCM7OCcyEwMys4FwIzs4JzITAzKzgXAjOzgnMhMDMruP8FHnKVt4/Ua94AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr_nb,tpr_nb)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff5a7c41a50>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now let's plot both models on the same plot so we can see how they compare\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr,tpr, label='log_reg')\n",
    "plt.plot(fpr_nb,tpr_nb, label='nb')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression is VERY slightly better than Naive Bayes for this problem. And once again, we have not done any tuning to these models, but likely we would want to lean towards tho logistic regression.\n",
    "\n",
    "Now, we cannot use the same method to evaluate KNN, simply because it's not a probability-based model - it uses distance. While we want to find the optimal probability threshold for NB or Logistic Regression, for KNN we would be looking for the optimal number of neighbours. To compare it to these two models though, we could use the confusion matrix, recall, precision, and F1 score. And the roc_curve which is based on predicted values, rather than probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff5a7d4cc90>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, yhat_knn)\n",
    "plt.plot(fpr_knn,tpr_knn, label='knn')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn F1:  0.5538461538461539\n",
      "knn precision:  0.5538461538461539\n",
      "knn precision:  0.5538461538461539\n",
      "knn precision:  0.6759776536312849\n"
     ]
    }
   ],
   "source": [
    "print(\"knn F1: \", f1_score(y_test,yhat_knn))\n",
    "print(\"knn precision: \", precision_score(y_test,yhat_knn))\n",
    "print(\"knn precision: \", recall_score(y_test,yhat_knn))\n",
    "print(\"knn precision: \", accuracy_score(y_test,yhat_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall this model performed the worst, so... we should just scrap it. AND THAT'S IT! I hope this was helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec34e64f2b95dc7183eda31f4b1ab04e3712d1753bd3e78e25e6efc4f82ffea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
